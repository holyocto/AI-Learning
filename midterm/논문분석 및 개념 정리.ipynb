{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00016781",
   "metadata": {},
   "source": [
    "## 📊 논문 기반 모델 세부사항 정리\n",
    "\n",
    "| 항목 | 설정값 / 설명 |\n",
    "|------|----------------|\n",
    "| **입력 이미지 크기** | 128 × 128 (Grayscale, 1채널) |\n",
    "| **모델 구조** | LeNet-5 모델 3개 병렬 구성 후 Concatenation |\n",
    "| **Conv1** | 6 filters, kernel size (5×5), ReLU, padding=same |\n",
    "| **Pooling1** | Average Pooling (2×2) |\n",
    "| **Conv2** | 16 filters, kernel size (5×5), ReLU |\n",
    "| **Pooling2** | Average Pooling (2×2) |\n",
    "| **FC1 (각 LeNet)** | 120 neurons, ReLU |\n",
    "| **FC2 (각 LeNet)** | 84 neurons, ReLU |\n",
    "| **출력층 (각 LeNet)** | 2 neurons, Softmax |\n",
    "| **합치기 방식** | 세 개의 LeNet 출력 → Concatenate → Reshape(nets, 2) → 최종 Softmax |\n",
    "| **총 모델 수** | 3 (병렬 LeNet-5) |\n",
    "| **Optimizer** | Adam |\n",
    "| **Loss Function** | Categorical Crossentropy |\n",
    "| **Activation Function** | Modified ReLU |\n",
    "| **정규화 기법** | Batch Normalization (각 Conv 뒤에 적용) |\n",
    "| **Dropout** | ❌ 사용 여부 언급 없음 (논문 미기재) |\n",
    "| **Epoch 수** | ❌ 명시되지 않음 (그래프 기반 추정 필요) |\n",
    "| **데이터 분할** | Train: 4684, Validation: 1152, Test: 20 |\n",
    "| **프레임워크** | Keras 2.6.0, TensorFlow 2.14.0 (Google Colab T4 GPU 사용) |\n",
    "| **정확도 (논문 기준)** | Train: 99%, Test: 96% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbad29b",
   "metadata": {},
   "source": [
    "---\n",
    "### 📊 Validation Accuracy 안정화\n",
    "\n",
    "| 현상 | 의미 | 해석 |\n",
    "|------|------|------|\n",
    "| `train acc ↑`, `val acc ↑` | 학습도 잘 되고, 일반화도 잘 되고 있음 | 🔥 Best case |\n",
    "| `train acc ↑`, `val acc` 요동 → 이후 안정화 | 점차 일반화되는 학습 흐름 | 👍 긍정적 흐름 |\n",
    "| `train acc ↑`, `val acc ↓ (지속)` | 과적합 발생 | ⚠️ Early stopping 또는 regularization 필요 |\n",
    "| `val acc ↑↓ 반복`, `train acc`도 불안정 | 학습률, 초기 가중치, 데이터 품질 문제 가능성 | 🛠️ 튜닝 필요 |\n",
    "\n",
    "#### ✅ 결론\n",
    "- Validation accuracy가 **안정화되면 모델이 일반적인 패턴을 잘 학습하고 있다는 신호**\n",
    "- 초기의 출렁임은 흔한 일이며, **이후 수렴 여부**가 더 중요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28b6f",
   "metadata": {},
   "source": [
    "---\n",
    "### 🧠 Dropout이란?\n",
    "\n",
    "**Dropout**은 신경망 학습 과정에서 일부 뉴런을 무작위로 꺼서(overfitting을 막는) 정규화 기법\n",
    "\n",
    "> 학습 시 일부 뉴런을 무작위로 제거함으로써, 특정 경로에 의존하지 않고 일반화된 모델을 만들 수 있게 함\n",
    "\n",
    "#### 🔍 사용 이유\n",
    "\n",
    "- 모델이 학습 데이터에 너무 **과적합(overfitting)** 되는 것을 방지\n",
    "- 다양한 뉴런 조합을 학습하게 하여 **robust한 성능** 유도\n",
    "- 테스트 시에는 모든 뉴런을 사용하여 **안정적인 예측** 수행\n",
    "\n",
    "#### 🧪 PyTorch 코드 예시\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% 확률로 뉴런 끔\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
